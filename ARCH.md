Architecture Overview: Tally Job Scheduler
1. Introduction
This document describes the architecture of the Tally Job Scheduler, a system designed to accept, process, and monitor asynchronous background jobs. The system provides users with real-time feedback on job progress through a WebSocket interface.

The core principles of this architecture are:

Decoupling: The web-facing API is separate from the background processing logic.

Scalability: Components can be scaled independently to handle increased load.

Reliability: A dedicated task queue ensures that jobs are not lost and can be retried.

Real-time Feedback: Users receive immediate updates on the status of their submitted jobs.

2. Core Components
The system is composed of four primary services:

A. Job Submission API
Responsibility: Provides an HTTP interface for users to submit new jobs, check the status of existing jobs, and establish a real-time notification connection.

Technology Stack:

Framework: FastAPI (Python) for its high performance and native async support, which is ideal for handling WebSockets.

Validation: Pydantic (integrated into FastAPI) for robust request data validation.

Key Interactions:

Receives job submissions from users via POST /jobs.

Writes new jobs to the PostgreSQL Database with an initial status of pending.

Manages persistent WebSocket connections for real-time client updates.

B. PostgreSQL Database
Responsibility: Acts as the single source of truth for all system data.

Technology Stack:

Database: PostgreSQL for its reliability, performance, and strong support for JSON data types (for job payloads and logs).

ORM: SQLModel (or SQLAlchemy) to provide a clean, Pythonic interface to the database.

Key Interactions:

Stores all jobs and their current status (pending, running, completed, failed).

Stores detailed logs generated by the Background Worker.

Read by the API to serve status requests.

Polled and updated continuously by the Background Worker.

C. Background Worker (Simulator)
Responsibility: The processing engine of the system. It fetches pending jobs from the database and executes them.

Technology Stack:

Task Queue: Celery for its robust, distributed task queue capabilities.

Message Broker: Redis or RabbitMQ to mediate communication between the API (task publisher) and Celery workers (task consumers). Redis is also used for its Pub/Sub feature.

Key Interactions:

Continuously polls the PostgreSQL Database for jobs with status = 'pending'.

Updates the job status to running upon starting a task.

Simulates work through a series of steps.

Writes execution logs to the PostgreSQL Database.

Publishes status update messages to the Real-time Notification Service (via Redis Pub/Sub).

Updates the final job status to completed or failed in the database.

D. Real-time Notification Service
Responsibility: Pushes job status updates to the correct client over a WebSocket connection. This service is logically separate but co-located within the Job Submission API process.

Technology Stack:

WebSockets: Managed via FastAPI's WebSocket support.

Message Bus: Redis Pub/Sub to receive messages from the Background Worker in a decoupled manner.

Key Interactions:

The API maintains a mapping of connected clients (e.g., user_id -> websocket_connection).

It subscribes to a Redis Pub/Sub channel (e.g., job_updates).

When the Background Worker publishes an update, this service receives it and forwards the message to the appropriate client through their WebSocket connection.

3. System Workflow / Data Flow
This diagram illustrates the complete lifecycle of a job from submission to completion.

Code snippet

graph TD
    subgraph Client
        U[User's Browser/Client]
    end

    subgraph "API Server (FastAPI)"
        A[/"POST /jobs" endpoint]
        WSS[WebSocket Manager]
    end

    subgraph "Task Queue System"
        B[/"Celery Worker (Simulator)"]
        R_PS[("Redis Pub/Sub")]
    end

    subgraph "Persistence Layer"
        DB[(PostgreSQL Database)]
    end


    U -- 1. Submit Job (HTTP) --> A
    A -- 2. Create Job (status: pending) --> DB
    
    U -- 3. Open Connection (WebSocket) --> WSS
    WSS -- 4. Subscribes to updates --> R_PS

    B -- 5. Polls for pending jobs --> DB
    B -- 6. Update status: 'running' --> DB
    B -- 7. Publish "RUNNING" update --> R_PS
    R_PS -- 8. Forwards message --> WSS
    WSS -- 9. Push "RUNNING" to client --> U

    B -- 10. Process job & write logs --> DB
    
    B -- 11. Update status: 'completed' --> DB
    B -- 12. Publish "COMPLETED" update --> R_PS
    R_PS -- 13. Forwards message --> WSS
    WSS -- 14. Push "COMPLETED" to client --> U

Step-by-Step Breakdown:

Job Submission: A user sends an HTTP POST request to the /jobs endpoint of the Job Submission API.

Persistence: The API validates the request, creates a new Job record in the PostgreSQL Database with a default status of pending.

Real-time Connection: The user's client establishes a WebSocket connection with the WebSocket Manager on the API server to listen for updates for their jobs.

Subscription: The WebSocket Manager subscribes to a Redis Pub/Sub channel to listen for notifications from the background workers.

Job Discovery: A Celery Worker, running in the background, polls the database for jobs in the pending state. It picks one to process.

Status Update (Running): The worker immediately updates the job's status in the database from pending to running.

Publish Update: The worker publishes a JSON message (e.g., {"job_id": "xyz", "status": "running"}) to the job_updates Redis channel.

Forward Update: The WebSocket Manager, which is listening to the channel, receives the message.

Push to Client: The manager identifies the correct client and sends the status update through their established WebSocket connection.

Processing & Logging: The worker executes the "fake" steps of the simulation. For each significant step, it inserts a new row into the job_logs table in the database, linked to the job_id.

Status Update (Completed): Once processing is finished, the worker updates the job's status in the database to completed (or failed).

Publish Final Update: The worker publishes a final status update message to the Redis channel.

Forward Final Update: The WebSocket Manager receives the final message.

Push Final Update: The manager sends the final status to the client, which can now display the completed state and close the connection if desired.

4. Data Models
Two primary tables are required to support this architecture.

jobs Table

Column Name

Data Type

Description

id

UUID (Primary Key)

Unique identifier for the job.

status

VARCHAR

Current state: pending, running, completed, failed.

priority

INTEGER

Job priority for ordering.

payload

JSONB

Input data and parameters for the job.

resource_requirements

JSONB

CPU/Memory needed for the job.

created_at

TIMESTAMP WITH TIME ZONE

Timestamp of job submission.

updated_at

TIMESTAMP WITH TIME ZONE

Timestamp of the last status update.

started_at

TIMESTAMP WITH TIME ZONE

Timestamp when processing began.

completed_at

TIMESTAMP WITH TIME ZONE

Timestamp when processing finished.


Export to Sheets
job_logs Table

Column Name

Data Type

Description

id

BIGSERIAL (Primary Key)

Unique identifier for the log entry.

job_id

UUID (Foreign Key)

Links the log to the corresponding job.

timestamp

TIMESTAMP WITH TIME ZONE

Time the log event occurred.

message

TEXT

The log message content.

level

VARCHAR

Log level, e.g., INFO, DEBUG, ERROR.


Export to Sheets
5. Scalability & Considerations
API Layer: The FastAPI server can be replicated behind a load balancer to handle a high volume of incoming requests.

Worker Layer: The number of Celery workers can be increased or decreased based on the job queue length, allowing the system to scale its processing power dynamically.

Database: PostgreSQL can be scaled using read replicas for read-heavy operations or sharding for write-heavy operations, although this adds significant complexity.

Error Handling: The Celery worker must have robust try...except blocks to catch errors during job execution, log them, and update the job status to failed. Celery's built-in retry mechanisms can be used for transient failures.